{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from inspect_ai import Task, task\n",
    "from inspect_ai.dataset import Sample, hf_dataset\n",
    "from inspect_ai.model import GenerateConfig\n",
    "from inspect_ai.scorer import choice\n",
    "from inspect_ai.solver import (\n",
    "    Solver,\n",
    "    multiple_choice,\n",
    "    system_message,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANSWER_TO_LETTER = {\"1\": \"A\", \"2\": \"B\"}\n",
    "BLANK_TAG = \"[BLANK]\"\n",
    "\n",
    "# Based on the prompt provided here:\n",
    "# https://huggingface.co/datasets/meta-llama/Meta-Llama-3.1-8B-evals/viewer/Meta-Llama-3.1-8B-evals__winogrande__details?row=0\n",
    "SYSTEM_W_EXAMPLES_PROMPT_TEMPLATE = (\n",
    "    f\"\"\"The following are multiple choice questions, with answers on the best logical completion to replace {BLANK_TAG} by {list(ANSWER_TO_LETTER.values())[0]} or {list(ANSWER_TO_LETTER.values())[1]}.\"\"\"\n",
    "    + \"\"\"\\n\\n{examples}\\n\"\"\"\n",
    ")\n",
    "# Based on MultipleChoiceTemplate.SINGLE_ANSWER provided in the multiple choice solver:\n",
    "# https://github.com/UKGovernmentBEIS/inspect_ai/blob/main/src/inspect_ai/solver/_multiple_choice.py\n",
    "USER_PROMPT_TEMPLATE = (\n",
    "    f\"\"\"Answer the following multiple choice question by choosing the best logical option to replace the {BLANK_TAG}.\"\"\"\n",
    "    + \"\"\" The entire content of your response should be of the following format: 'ANSWER: $LETTER' (without quotes) where LETTER is one of {letters}.\\n\\n{question}\\n{choices}\\n\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_to_sample(record: dict[str, Any]) -> Sample:\n",
    "    input = f\"\"\"Sentence: {record[\"sentence\"].replace(\"_\", BLANK_TAG)}\"\"\"\n",
    "    target = ANSWER_TO_LETTER[record[\"answer\"]]\n",
    "    choices = [record[\"option1\"], record[\"option2\"]]  # Order is IMP\n",
    "\n",
    "    return Sample(\n",
    "        input=input,\n",
    "        choices=choices,\n",
    "        target=target,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_to_fewshot(sample: Sample) -> str:\n",
    "    sent_str = str(sample.input)\n",
    "    choices = sample.choices if sample.choices is not None else []\n",
    "    assert (\n",
    "        len(choices) == 2\n",
    "    ), \"Number of choices should be 2 for the winogrande dataset.\"\n",
    "    opt1_str = f\"\"\"{list(ANSWER_TO_LETTER.values())[0]}) {choices[0]}\"\"\"\n",
    "    opt2_str = f\"\"\"{list(ANSWER_TO_LETTER.values())[1]}) {choices[1]}\"\"\"\n",
    "    ans_str = f\"\"\"ANSWER: {sample.target}\"\"\"\n",
    "    final_str = \"\\n\".join([sent_str, opt1_str, opt2_str, ans_str])\n",
    "\n",
    "    return final_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def winogrande_solver(\n",
    "    dataset_name: str,\n",
    "    fewshot: int,\n",
    "    fewshot_seed: int,\n",
    ") -> list[Solver]:\n",
    "    solver = [multiple_choice(template=USER_PROMPT_TEMPLATE, shuffle=False)]\n",
    "\n",
    "    if fewshot:\n",
    "        fewshot_samples = hf_dataset(\n",
    "            \"allenai/winogrande\",\n",
    "            name=dataset_name,\n",
    "            split=\"train\",\n",
    "            trust=True,\n",
    "            sample_fields=record_to_sample,\n",
    "            auto_id=True,\n",
    "            shuffle=True,\n",
    "            seed=fewshot_seed,\n",
    "            limit=fewshot,\n",
    "        )\n",
    "        solver.insert(\n",
    "            0,\n",
    "            system_message(\n",
    "                SYSTEM_W_EXAMPLES_PROMPT_TEMPLATE.format(\n",
    "                    examples=\"\\n\\n\".join(\n",
    "                        [sample_to_fewshot(sample=sample) for sample in fewshot_samples]\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    return solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def winogrande(\n",
    "    dataset_name: str = \"winogrande_xl\",\n",
    "    fewshot: int = 5,\n",
    "    fewshot_seed: int = 42,\n",
    ") -> Task:\n",
    "    \"\"\"Inspect task implementing the WinoGrande benchmark.\n",
    "\n",
    "    Arguments:\n",
    "        dataset_name (str): Subset of the dataset to be used.\n",
    "        fewshot (int): Number of few shot examples to use.\n",
    "        fewshot_seed (int): Random seed for sampling few shot examples.\n",
    "    \"\"\"\n",
    "    return Task(\n",
    "        dataset=hf_dataset(\n",
    "            \"allenai/winogrande\",\n",
    "            name=dataset_name,\n",
    "            split=\"validation\",\n",
    "            trust=True,\n",
    "            sample_fields=record_to_sample,\n",
    "        ),\n",
    "        solver=winogrande_solver(\n",
    "            dataset_name=dataset_name, fewshot=fewshot, fewshot_seed=fewshot_seed\n",
    "        ),\n",
    "        scorer=choice(),\n",
    "        config=GenerateConfig(max_tokens=64),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
